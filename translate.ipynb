{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:24.285334Z",
     "start_time": "2025-05-04T11:54:24.280980Z"
    }
   },
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import random\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:24.315560Z",
     "start_time": "2025-05-04T11:54:24.312663Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "7e47595bdf3e42bf",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:24.335470Z",
     "start_time": "2025-05-04T11:54:24.330701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ],
   "id": "8f1b0f22c64f6438",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:24.352506Z",
     "start_time": "2025-05-04T11:54:24.348128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ],
   "id": "b3c42bc02e80e1bd",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:24.567329Z",
     "start_time": "2025-05-04T11:54:24.364690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# clean data as pol.txt contains CC-BY copyright in third column\n",
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('pol.txt', sep='\\t', header=None)\n",
    "df = df[[0, 1]]\n",
    "df.to_csv('eng-pol.txt', sep='\\t', header=False, index=False)"
   ],
   "id": "6c6943d450d8748a",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:24.584604Z",
     "start_time": "2025-05-04T11:54:24.580891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n"
   ],
   "id": "6ef1aea2fdac8a57",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:24.602771Z",
     "start_time": "2025-05-04T11:54:24.597972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am\", \"i m\",\n",
    "    \"he is\", \"he s\",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re\",\n",
    "    \"we are\", \"we re\",\n",
    "    \"they are\", \"they re\",\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ],
   "id": "356222e521807848",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:25.456023Z",
     "start_time": "2025-05-04T11:54:24.617460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(\"eng\", \"pol\", True)\n",
    "print(random.choice(pairs))"
   ],
   "id": "3729e23f946f2283",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 49943 sentence pairs\n",
      "Trimmed to 4021 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "pol 3435\n",
      "eng 2212\n",
      "['martwie sie troche o tom a', 'i m little worried about tom']\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:25.484912Z",
     "start_time": "2025-05-04T11:54:25.481598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ],
   "id": "62f5e0c2281c6532",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:25.502472Z",
     "start_time": "2025-05-04T11:54:25.496615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=1)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n"
   ],
   "id": "1a7a46eab1bdda11",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:25.516147Z",
     "start_time": "2025-05-04T11:54:25.507785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights\n"
   ],
   "id": "d0e90185dfe91bb4",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:25.541531Z",
     "start_time": "2025-05-04T11:54:25.535688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'pol', True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device), torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ],
   "id": "acda2c2ae0ef2cf6",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:25.558425Z",
     "start_time": "2025-05-04T11:54:25.554277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "97ae27588b04622",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:25.578944Z",
     "start_time": "2025-05-04T11:54:25.574443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / percent\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ],
   "id": "5c15b4f7f021c70a",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:25.614841Z",
     "start_time": "2025-05-04T11:54:25.610349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ],
   "id": "5cc51cd2c610a4cf",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:25.630436Z",
     "start_time": "2025-05-04T11:54:25.627066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ],
   "id": "cd93b4d08c5dd07f",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:25.658089Z",
     "start_time": "2025-05-04T11:54:25.654248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoder_ids = topi.squeeze()\n",
    "\n",
    "        decoder_words = []\n",
    "        for idx in decoder_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoder_words.append('<EOS>')\n",
    "                break\n",
    "            decoder_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoder_words, decoder_attn"
   ],
   "id": "598a25c84fbbcbc",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:54:25.674683Z",
     "start_time": "2025-05-04T11:54:25.670398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ],
   "id": "83b3c1f2608ef50",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:59:41.609589Z",
     "start_time": "2025-05-04T11:54:25.695498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ],
   "id": "70cf691ba5b4e4b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 49943 sentence pairs\n",
      "Trimmed to 4021 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "pol 3435\n",
      "eng 2212\n",
      "0m 18s (- 4m 44s) (5 6%) 6.8667\n",
      "0m 40s (- 4m 42s) (10 12%) 6.8135\n",
      "1m 3s (- 4m 36s) (15 18%) 6.8009\n",
      "1m 23s (- 4m 10s) (20 25%) 6.7897\n",
      "1m 42s (- 3m 44s) (25 31%) 6.7787\n",
      "2m 1s (- 3m 23s) (30 37%) 6.7705\n",
      "2m 21s (- 3m 2s) (35 43%) 6.7611\n",
      "2m 41s (- 2m 41s) (40 50%) 6.7529\n",
      "3m 3s (- 2m 22s) (45 56%) 6.7458\n",
      "3m 24s (- 2m 2s) (50 62%) 6.7374\n",
      "3m 43s (- 1m 41s) (55 68%) 6.7300\n",
      "4m 1s (- 1m 20s) (60 75%) 6.7124\n",
      "4m 20s (- 1m 0s) (65 81%) 6.7003\n",
      "4m 38s (- 0m 39s) (70 87%) 6.6894\n",
      "4m 56s (- 0m 19s) (75 93%) 6.6787\n",
      "5m 14s (- 0m 0s) (80 100%) 6.6752\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:59:41.688036Z",
     "start_time": "2025-05-04T11:59:41.627567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ],
   "id": "e05b6eb244e96450",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> nie jestem zazdrosna o toma\n",
      "= i m not jealous of tom\n",
      "< german SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> w niedziele mam wolne\n",
      "= i m free on sunday\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> jestes wyczerpany\n",
      "= you re exhausted\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> jestesmy teraz bardzo zajeci\n",
      "= we re very busy right now\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> on jest bardzo wrazliwy na zimno\n",
      "= he is very sensitive to cold\n",
      "< from SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> dzis wieczorem lece do australii\n",
      "= i m leaving tonight for australia\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> jest wystarczajaco bogaty zeby kupic ten obraz\n",
      "= he is rich enough to buy the painting\n",
      "< from credible SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> on gra muzyke\n",
      "= he is playing music\n",
      "< some SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> nie wszyscy tutaj sie uczymy\n",
      "= we re not all students here\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> jestes taktowny\n",
      "= you re tactful\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:59:41.706063Z",
     "start_time": "2025-05-04T11:59:41.701223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "    print('input = ', input_sentence)\n",
    "    print('output = ', ' '.join(output_words))\n",
    "\n",
    "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])"
   ],
   "id": "66fecf8f5a169796",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T12:29:10.948077Z",
     "start_time": "2025-05-04T12:29:10.815741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluateAndShowAttention('jest wystarczajaco bogaty zeby kupic ten obraz')\n",
    "\n",
    "evaluateAndShowAttention('on gra muzyke')\n",
    "\n",
    "evaluateAndShowAttention('jestes taktowny')\n",
    "\n",
    "evaluateAndShowAttention('nie wszyscy tutaj sie uczymy')"
   ],
   "id": "983750d6fc572244",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input =  jest wystarczajaco bogaty zeby kupic ten obraz\n",
      "output =  from credible SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "input =  on gra muzyke\n",
      "output =  some SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "input =  jestes taktowny\n",
      "output =  SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "input =  nie wszyscy tutaj sie uczymy\n",
      "output =  SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_24524\\3122597877.py:7: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_24524\\3122597877.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_24524\\3122597877.py:13: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_24524\\3122597877.py:7: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_24524\\3122597877.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_24524\\3122597877.py:13: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_24524\\3122597877.py:7: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_24524\\3122597877.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_24524\\3122597877.py:13: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_24524\\3122597877.py:7: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_24524\\3122597877.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "C:\\Users\\mike\\AppData\\Local\\Temp\\ipykernel_24524\\3122597877.py:13: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T12:54:18.704415Z",
     "start_time": "2025-05-04T12:54:18.669489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save model\n",
    "\n",
    "torch.save(encoder.state_dict(), 'encoder.dict')\n",
    "torch.save(decoder.state_dict(), 'decoder.dict')"
   ],
   "id": "24626e97da3ef34b",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T12:54:22.244003Z",
     "start_time": "2025-05-04T12:54:22.199652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load('encoder.dict'))\n",
    "decoder.load_state_dict(torch.load('decoder.dict'))"
   ],
   "id": "757e83bc97931b20",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T12:54:23.849920Z",
     "start_time": "2025-05-04T12:54:23.796390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ],
   "id": "f872c5e3f06434e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> jestes pomocny\n",
      "= you re helpful\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> krzykna do niej zeby uwaza a\n",
      "= he shouted to her to be careful\n",
      "< true SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> nie zwraca uwagi na nauczyciela\n",
      "= he s not paying attention to the teacher\n",
      "< traffic mean mean mean mean mean mean mean mean mean\n",
      "\n",
      "> ochryp em od zbyt intensywnego krzyku\n",
      "= i am hoarse from yelling so much\n",
      "< i i obese SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> jestem pewien ze tom jest na mnie wsciek y\n",
      "= i m sure tom is mad at me\n",
      "< i i i i department sure sure sure SOS SOS\n",
      "\n",
      "> przyszed em kilka minut za wczesnie\n",
      "= i m a few minutes early\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> my pomozemy tomowi\n",
      "= we re going to help tom\n",
      "< nonsmokers SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> planujemy zostac tam tydzien\n",
      "= we re planning to stay for a week\n",
      "< contributing SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> ide do miasta\n",
      "= i m going to town\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> pojde umyc rece\n",
      "= i m going to wash my hands\n",
      "< safety safety self SOS SOS SOS SOS SOS SOS SOS\n",
      "\n"
     ]
    }
   ],
   "execution_count": 80
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
